{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add, Flatten, Lambda, Reshape, BatchNormalization, concatenate, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError, RootMeanSquaredError, Mean, binary_crossentropy, binary_accuracy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     import google.colab\n",
    "#     in_colab = True\n",
    "# except ImportError:\n",
    "#     in_colab = False\n",
    "\n",
    "# if in_colab:\n",
    "#     try:\n",
    "#         tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#         print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "#         tf.config.experimental_connect_to_cluster(tpu)\n",
    "#         tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#         tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
    "#     except ValueError:\n",
    "#         if 'GPU' in [gpu[-1] for gpu in tf.config.list_physical_devices('GPU')]:\n",
    "#             print('Running on Google Colab GPU')\n",
    "#         else:\n",
    "#             print('Not connected to a TPU or GPU runtime')\n",
    "\n",
    "# else:\n",
    "#     physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#     print(physical_devices)\n",
    "#     try:\n",
    "#         if len(physical_devices):\n",
    "#             for gpu in physical_devices:\n",
    "#                 tf.config.experimental.set_memory_growth(gpu, enable = True)\n",
    "#                 print(f\"Using local GPU: {gpu}\")\n",
    "#             sess = tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(log_device_placement = True))\n",
    "#         else:\n",
    "#             print(\"No local GPU found\")\n",
    "#     except:\n",
    "#         print(\"Error Mounting a GPU\")\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors and perform an operation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(input_shape=(800, 1200, 3)):\n",
    "    # Input layer\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    conv1 = Conv2D(8, (3, 3), padding='same', activation='elu')(input_img)\n",
    "    conv2 = Conv2D(16, (3, 3), padding='same', activation='elu')(conv1)\n",
    "    conv3 = Conv2D(32, (3, 3), padding='same', activation='elu')(conv2)\n",
    "    conv4 = Conv2D(64, (3, 3), padding='same', activation='elu')(conv3)\n",
    "    concat1 = concatenate([conv2, conv4], axis=-1)\n",
    "    conv5 = Conv2D(64, (3, 3), padding='same', activation='elu')(concat1)\n",
    "    output_img = Conv2D(3, (3, 3), padding='same', activation='sigmoid')(conv5)\n",
    "\n",
    "    # Create the model\n",
    "    generator_model = Model(inputs=input_img, outputs=output_img)\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 800, 1200,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 800, 1200, 8  224         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 800, 1200, 1  1168        ['conv2d[0][0]']                 \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 800, 1200, 3  4640        ['conv2d_1[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 800, 1200, 6  18496       ['conv2d_2[0][0]']               \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 800, 1200, 8  0           ['conv2d_1[0][0]',               \n",
      "                                0)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 800, 1200, 6  46144       ['concatenate[0][0]']            \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 800, 1200, 3  1731        ['conv2d_4[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 72,403\n",
      "Trainable params: 72,403\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(input_shape=(800, 1200, 3)):\n",
    "    # Input layer\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    conv1 = Conv2D(8, (7, 7), activation='relu')(input_img)\n",
    "    conv2 = Conv2D(16, (7, 7), activation='relu')(conv1)\n",
    "    conv3 = Conv2D(32, (7, 7), activation='relu')(conv2)\n",
    "    conv4 = Conv2D(4, (7, 7), activation='relu')(conv3)\n",
    "    maxpool1 = MaxPooling2D((7, 7))(conv4)\n",
    "    flatten = Flatten()(maxpool1)\n",
    "    output_img = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "    # Create the model\n",
    "    discriminator_model = Model(inputs=input_img, outputs=output_img)\n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 800, 1200, 3)]    0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 794, 1194, 8)      1184      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 788, 1188, 16)     6288      \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 782, 1182, 32)     25120     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 776, 1176, 4)      6276      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 110, 168, 4)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 73920)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 73921     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,789\n",
      "Trainable params: 112,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X, y, num_examples=5, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.randint(0, len(X), num_examples)\n",
    "    for i in indices:\n",
    "        x_sample = y[i:i+1]\n",
    "        y_sample = y[i:i+1]\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(x_sample)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(x_sample[0], cmap='gray')\n",
    "        plt.title('Original')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(y_sample[0], cmap='gray')\n",
    "        plt.title('Expected')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(y_pred[0], cmap='gray')\n",
    "        plt.title('Reconstructed by Model')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 validated image filenames.\n",
      "Found 20 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'Data/image_data.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "low_res_folder = 'Data/low res/'\n",
    "high_res_folder = 'Data/high res/'\n",
    "\n",
    "df = df[df['low_res'].str.contains('_6')]\n",
    "\n",
    "df = df.iloc[:100]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=low_res_folder,\n",
    "    x_col='low_res',\n",
    "    y_col='high_res',\n",
    "    target_size=(800, 1200),\n",
    "    seed=42,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode='input',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=low_res_folder,\n",
    "    x_col='low_res',\n",
    "    y_col='high_res',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size=(800, 1200),\n",
    "    seed=42,\n",
    "    class_mode='input',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/100 [00:00<?, ?epoch/s]c:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "Training Epochs:   0%|          | 0/100 [24:01<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5756\\2237029185.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;31m# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mearly_stopping_patience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    888\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[1;31m# Implements GenericFunction.__call__.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_functions_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_function_call\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"eager\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;31m# Only count the statistics the first time, before initialization took\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[1;31m# place.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5756\\2237029185.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdisc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mgradients_of_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mgradients_of_discriminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients_of_discriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1109\u001b[0m               output_gradients))\n\u001b[0;32m   1110\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    587\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           data_format=data_format),\n\u001b[1;32m--> 591\u001b[1;33m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[0;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barte\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1264\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n\u001b[0;32m   1267\u001b[0m           \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "avr_train_gen_losses = []\n",
    "avr_train_disc_losses = []\n",
    "val_gen_losses = []\n",
    "val_disc_losses = []\n",
    "\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, y):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator(y, training=True)\n",
    "        generated_images = generator(X, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    train_gen_losses.append(gen_loss)\n",
    "    train_disc_losses.append(disc_loss)\n",
    "\n",
    "    try:\n",
    "        mae_list.append(tf.abs(generated_images - y))\n",
    "        mse_list.append(tf.square(generated_images - y))\n",
    "        accuracy_list.append(tf.keras.metrics.binary_accuracy(generated_images, y))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Training Epochs\", unit=\"epoch\"):\n",
    "    for batch_X, batch_y in train_generator:\n",
    "        train_step(batch_X, batch_y)\n",
    "\n",
    "    val_losses = []\n",
    "    val_mae = []\n",
    "    val_mse = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for val_batch_X, val_batch_y in validation_generator:\n",
    "        val_real_output = discriminator(val_batch_y, training=False)\n",
    "        val_generated_images = generator(val_batch_X, training=False)\n",
    "        val_fake_output = discriminator(val_generated_images, training=False)\n",
    "        val_gen_loss = generator_loss(val_fake_output)\n",
    "        val_disc_loss = discriminator_loss(val_real_output, val_fake_output)\n",
    "        val_losses.append((val_gen_loss, val_disc_loss))\n",
    "\n",
    "    val_mae.append(np.abs(val_real_output.numpy() - val_fake_output.numpy()))\n",
    "    val_mse.append(np.square(val_real_output.numpy() - val_fake_output.numpy()))\n",
    "    val_accuracy.append(tf.keras.metrics.binary_accuracy(val_real_output.numpy(), val_fake_output.numpy()))\n",
    "\n",
    "    avg_val_gen_loss = tf.reduce_mean([loss[0] for loss in val_losses])\n",
    "    avg_val_disc_loss = tf.reduce_mean([loss[1] for loss in val_losses])\n",
    "    avr_train_gen_losses.append(tf.reduce_mean(train_gen_losses))\n",
    "    avr_train_disc_losses.append(tf.reduce_mean(train_disc_losses))\n",
    "\n",
    "    mae = tf.reduce_mean(mae_list)\n",
    "    mse = tf.reduce_mean(mse_list)\n",
    "    accuracy = tf.reduce_mean(accuracy_list)\n",
    "    val_gen_losses.append(avg_val_gen_loss)\n",
    "    val_disc_losses.append(avg_val_disc_loss)\n",
    "\n",
    "    val_mae = np.array(val_mae)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, \"\n",
    "          f\"Average Training Generator Loss: {avr_train_gen_losses[-1]}, \"\n",
    "          f\"Average Training Discriminator Loss: {avr_train_disc_losses[-1]}, \"\n",
    "          f\"Average Validation Generator Loss: {avg_val_gen_loss}, \"\n",
    "          f\"Average Validation Discriminator Loss: {avg_val_disc_loss}, \"\n",
    "          f\"Average Training MAE: {mae}, \"\n",
    "          f\"Average Training MSE: {mse}, \"\n",
    "          f\"Average Training Accuracy: {accuracy}\"\n",
    "          f\"Average Validation MAE: {tf.reduce_mean(val_mae)}, \"\n",
    "          f\"Average Validation MSE: {tf.reduce_mean(val_mse)}, \"\n",
    "          f\"Average Validation Accuracy: {tf.reduce_mean(val_accuracy)}\",\n",
    "          )\n",
    "    \n",
    "    mae_list = []\n",
    "    mse_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    if avg_val_gen_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_gen_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter == early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
    "        generator.load_weights('best_generator_weights.h5')\n",
    "        discriminator.load_weights('best_discriminator_weights.h5')\n",
    "        break\n",
    "\n",
    "    train_generator.reset()\n",
    "    validation_generator.reset()\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(avr_train_gen_losses, label='Training Generator Loss')\n",
    "plt.plot(avr_train_disc_losses, label='Training Discriminator Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_gen_losses, label='Validation Generator Loss')\n",
    "plt.plot(val_disc_losses, label='Validation Discriminator Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(generator, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(generator, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
